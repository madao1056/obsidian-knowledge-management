# 自動化ルール

## 概要
ObsidianとCursorの連携における自動化ルールと実装方法を定義します。

## 自動化の原則

### 1. DRY原則（Don't Repeat Yourself）
- 同じ作業を2回以上繰り返したら自動化
- テンプレート化またはスクリプト化

### 2. シンプルに保つ
- 複雑なロジックは避ける
- メンテナンスしやすいコード

### 3. エラーハンドリング
- 失敗しても安全に
- ログ出力でデバッグ可能に

## 現在の自動化

### 1. clipフォルダ自動整理

#### 実装
`process_clip.py`

#### 機能
- コンテンツタイプ判定
- メタデータ抽出
- 適切なフォルダへ移動
- フォーマット変換

#### 実行方法
```bash
# 手動実行
python 100_cursor/process_clip.py

# Cursorで実行
"クリップを整理して"
```

## 追加予定の自動化

### 2. Daily Note自動生成

#### 仕様
```python
# daily_note_generator.py
import datetime
from pathlib import Path

def create_daily_note():
    today = datetime.date.today()
    filename = f"{today.strftime('%Y%m%d')}_daily.md"
    template = get_template('daily_note')
    
    content = template.replace('{{date}}', today.strftime('%Y-%m-%d'))
    content = content.replace('{{yesterday}}', 
                             (today - datetime.timedelta(days=1)).strftime('%Y-%m-%d'))
    
    file_path = Path('00_Inbox') / filename
    
    if not file_path.exists():
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"Created: {file_path}")
```

### 3. タグ分析・整理

#### 仕様
```python
# tag_analyzer.py
import re
from collections import Counter
from pathlib import Path

def analyze_tags(vault_path):
    tag_counter = Counter()
    tag_locations = {}
    
    for file_path in Path(vault_path).glob("**/*.md"):
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        tags = re.findall(r'#[\w/]+', content)
        for tag in tags:
            tag_counter[tag] += 1
            if tag not in tag_locations:
                tag_locations[tag] = []
            tag_locations[tag].append(str(file_path))
    
    # タグ一覧を生成
    generate_tag_index(tag_counter, tag_locations)
```

### 4. リンクチェッカー

#### 仕様
```python
# link_checker.py
import re
from pathlib import Path

def check_links(vault_path):
    broken_links = []
    all_files = {f.stem: f for f in Path(vault_path).glob("**/*.md")}
    
    for file_path in all_files.values():
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # [[リンク]]形式を抽出
        links = re.findall(r'\[\[([^\]]+)\]\]', content)
        
        for link in links:
            link_name = link.split('|')[0]  # エイリアスを除去
            if link_name not in all_files:
                broken_links.append({
                    'file': str(file_path),
                    'link': link_name
                })
    
    return broken_links
```

### 5. MOC自動生成

#### 仕様
```python
# moc_generator.py
def generate_moc(topic, vault_path):
    """特定トピックのMOCを自動生成"""
    related_notes = find_related_notes(topic, vault_path)
    
    moc_content = f"""# {topic} MOC

## 概要
{topic}に関するノートの一覧

## ノート一覧
"""
    
    # カテゴリ別に整理
    categorized = categorize_notes(related_notes)
    
    for category, notes in categorized.items():
        moc_content += f"\n### {category}\n"
        for note in notes:
            moc_content += f"- [[{note['title']}]]\n"
    
    # MOCを保存
    moc_path = Path(vault_path) / '90_Index' / f'MOC_{topic}.md'
    with open(moc_path, 'w', encoding='utf-8') as f:
        f.write(moc_content)
```

## 自動化ワークフロー

### 1. 定期実行タスク

#### cron設定
```bash
# crontab -e
# 毎日6:00 - Daily Note作成
0 6 * * * cd /path/to/obsidian && python 100_cursor/daily_note_generator.py

# 毎日9:00, 13:00, 17:00 - clip整理
0 9,13,17 * * * cd /path/to/obsidian && python 100_cursor/process_clip.py

# 毎週日曜22:00 - タグ分析
0 22 * * 0 cd /path/to/obsidian && python 100_cursor/tag_analyzer.py

# 毎月初日 - リンクチェック
0 0 1 * * cd /path/to/obsidian && python 100_cursor/link_checker.py
```

### 2. イベントトリガー

#### Gitフック
```bash
#!/bin/sh
# .git/hooks/pre-commit

# clipフォルダ整理
if [ -d "00_Inbox/clip" ] && [ "$(ls -A 00_Inbox/clip)" ]; then
    echo "Processing clips..."
    python 100_cursor/process_clip.py
    git add 20_Literature/
fi

# リンクチェック
python 100_cursor/link_checker.py
if [ $? -ne 0 ]; then
    echo "Broken links found. Please fix before committing."
    exit 1
fi
```

#### ファイル監視
```python
# file_watcher.py
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import time

class ClipHandler(FileSystemEventHandler):
    def on_created(self, event):
        if event.src_path.endswith('.md') and '00_Inbox/clip' in event.src_path:
            time.sleep(1)  # ファイル書き込み完了待ち
            print(f"New clip detected: {event.src_path}")
            # process_clip.pyを実行

if __name__ == "__main__":
    observer = Observer()
    observer.schedule(ClipHandler(), path='00_Inbox/clip', recursive=False)
    observer.start()
    
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        observer.stop()
    observer.join()
```

### 3. バッチ処理

#### 週次メンテナンス
```python
# weekly_maintenance.py
def weekly_maintenance():
    """週次メンテナンスタスク"""
    tasks = [
        archive_old_inbox_items,
        consolidate_duplicate_notes,
        update_mocs,
        generate_weekly_report,
        cleanup_empty_folders
    ]
    
    for task in tasks:
        try:
            print(f"Running: {task.__name__}")
            task()
            print(f"Completed: {task.__name__}")
        except Exception as e:
            print(f"Error in {task.__name__}: {e}")
```

## エラーハンドリング

### ログシステム
```python
# logger.py
import logging
from datetime import datetime

def setup_logger(name):
    logger = logging.getLogger(name)
    logger.setLevel(logging.INFO)
    
    # ファイルハンドラー
    fh = logging.FileHandler(
        f'100_cursor/logs/{name}_{datetime.now().strftime("%Y%m%d")}.log'
    )
    fh.setLevel(logging.INFO)
    
    # フォーマット
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    fh.setFormatter(formatter)
    
    logger.addHandler(fh)
    return logger
```

### バックアップルール
```python
# backup.py
import shutil
from datetime import datetime

def backup_before_operation(file_path):
    """操作前にバックアップを作成"""
    backup_path = file_path.parent / 'backup' / 
                  f"{file_path.stem}_{datetime.now().strftime('%Y%m%d_%H%M%S')}{file_path.suffix}"
    backup_path.parent.mkdir(exist_ok=True)
    shutil.copy2(file_path, backup_path)
    return backup_path
```

## パフォーマンス最適化

### 並列処理
```python
# parallel_processor.py
from concurrent.futures import ThreadPoolExecutor, as_completed
import multiprocessing

def parallel_process_files(file_list, process_function):
    """複数ファイルを並列処理"""
    cpu_count = multiprocessing.cpu_count()
    with ThreadPoolExecutor(max_workers=cpu_count) as executor:
        futures = {executor.submit(process_function, f): f for f in file_list}
        
        for future in as_completed(futures):
            file_path = futures[future]
            try:
                result = future.result()
                print(f"Processed: {file_path}")
            except Exception as e:
                print(f"Error processing {file_path}: {e}")
```

### キャッシュ管理
```python
# cache_manager.py
import pickle
import hashlib
from pathlib import Path

class CacheManager:
    def __init__(self, cache_dir='100_cursor/.cache'):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
    
    def get_cache_key(self, content):
        return hashlib.md5(content.encode()).hexdigest()
    
    def get(self, key):
        cache_file = self.cache_dir / f"{key}.pkl"
        if cache_file.exists():
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        return None
    
    def set(self, key, value):
        cache_file = self.cache_dir / f"{key}.pkl"
        with open(cache_file, 'wb') as f:
            pickle.dump(value, f)
```

## 今後の拡張計画

### 短期（0-3ヶ月）
1. Daily Note自動生成実装
2. タグ分析ツール完成
3. リンクチェッカー実装

### 中期（3-6ヶ月）
1. AI統合（サマリー、タグ提案）
2. プラグイン化
3. Web UI作成

### 長期（6ヶ月以上）
1. 機械学習による分類
2. 自然言語処理
3. 知識グラフ構築

## 関連ドキュメント
- [[102_ワークフロー]]
- [[102-02_効率化テクニック]]
- [[process_clip.py]]