#!/usr/bin/env python3
"""
ObsidianãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹å†…ã®é–¢é€£æ€§ã‚’åˆ†æã—ã€ã‚¿ã‚°ã¨ãƒªãƒ³ã‚¯ã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import os
import re
import json
from datetime import datetime
from collections import defaultdict, Counter
import sys

# æ—¥æœ¬èªã®å½¢æ…‹ç´ è§£æ
try:
    import MeCab
    mecab_available = True
    mecab = MeCab.Tagger()
except ImportError:
    mecab_available = False
    print("MeCabãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚åŸºæœ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã®ã¿è¡Œã„ã¾ã™ã€‚")

# è¨­å®š
KNOWLEDGE_DIR = "/Users/hashiguchimasaki/project/obsidian"
NOTION_DIR = os.path.join(KNOWLEDGE_DIR, "20_Literature/25_Notion")
OUTPUT_DIR = os.path.join(KNOWLEDGE_DIR, "100_cursor/knowledge_analysis")

# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚«ãƒ†ã‚´ãƒªå®šç¾©
KEYWORD_CATEGORIES = {
    "å–¶æ¥­ãƒ»ãƒ“ã‚¸ãƒã‚¹": [
        "å–¶æ¥­", "ã‚»ãƒ¼ãƒ«ã‚¹", "ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ³ã‚°", "é¡§å®¢", "ææ¡ˆ", "å•†è«‡",
        "ãƒªãƒ¼ãƒ‰", "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°", "é›†å®¢", "æˆç´„", "å¥‘ç´„", "å£²ä¸Š",
        "åç›Š", "åˆ©ç›Š", "ãƒ“ã‚¸ãƒã‚¹", "äº‹æ¥­", "èµ·æ¥­", "çµŒå–¶"
    ],
    "ã‚¹ã‚­ãƒ«ãƒ»èƒ½åŠ›": [
        "ã‚¹ã‚­ãƒ«", "èƒ½åŠ›", "æŠ€è¡“", "ãƒã‚¦ãƒã‚¦", "çŸ¥è­˜", "çµŒé¨“",
        "å­¦ç¿’", "æˆé•·", "å‘ä¸Š", "æ”¹å–„", "ç¿’å¾—", "ãƒã‚¹ã‚¿ãƒ¼",
        "ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«", "å°‚é–€", "ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ"
    ],
    "ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³": [
        "ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³", "å¯¾è©±", "ä¼šè©±", "ã‚„ã‚Šå–ã‚Š", "è¿”ä¿¡",
        "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸", "é€£çµ¡", "ç›¸è«‡", "è³ªå•", "å›ç­”", "ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯",
        "ãƒ—ãƒ¬ã‚¼ãƒ³", "èª¬æ˜", "ä¼ãˆã‚‹", "èã", "ç†è§£"
    ],
    "ãƒã‚¤ãƒ³ãƒ‰ã‚»ãƒƒãƒˆ": [
        "ãƒã‚¤ãƒ³ãƒ‰", "æ€è€ƒ", "è€ƒãˆæ–¹", "æ„è­˜", "å§¿å‹¢", "æ…‹åº¦",
        "ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³", "ã‚„ã‚‹æ°—", "æƒ…ç†±", "ç›®æ¨™", "ç›®çš„",
        "ãƒ“ã‚¸ãƒ§ãƒ³", "ä¾¡å€¤è¦³", "ä¿¡å¿µ", "å“²å­¦"
    ],
    "Webåˆ¶ä½œãƒ»æŠ€è¡“": [
        "Web", "ã‚µã‚¤ãƒˆ", "ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸", "ãƒ‡ã‚¶ã‚¤ãƒ³", "ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°",
        "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°", "é–‹ç™º", "å®Ÿè£…", "WordPress", "HTML", "CSS",
        "JavaScript", "React", "TypeScript", "API", "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹"
    ],
    "AIãƒ»è‡ªå‹•åŒ–": [
        "AI", "äººå·¥çŸ¥èƒ½", "ChatGPT", "Claude", "è‡ªå‹•åŒ–", "åŠ¹ç‡åŒ–",
        "DX", "ãƒ‡ã‚¸ã‚¿ãƒ«", "ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼", "ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³",
        "æ©Ÿæ¢°å­¦ç¿’", "ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°", "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"
    ],
    "æ•™è‚²ãƒ»ã‚³ãƒ¼ãƒãƒ³ã‚°": [
        "æ•™è‚²", "æŒ‡å°", "ã‚³ãƒ¼ãƒãƒ³ã‚°", "ãƒ¡ãƒ³ã‚¿ãƒªãƒ³ã‚°", "ã‚µãƒãƒ¼ãƒˆ",
        "ã‚¢ãƒ‰ãƒã‚¤ã‚¹", "ã‚«ã‚¦ãƒ³ã‚»ãƒªãƒ³ã‚°", "ç›¸è«‡", "å£æ‰“ã¡", "å€‹åˆ¥",
        "ã‚»ãƒŸãƒŠãƒ¼", "è¬›åº§", "ç ”ä¿®", "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°"
    ]
}

def ensure_output_dir():
    """å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
    os.makedirs(OUTPUT_DIR, exist_ok=True)

def extract_keywords_basic(text):
    """åŸºæœ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º"""
    keywords = []
    
    # ã‚«ãƒ†ã‚´ãƒªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
    for category, words in KEYWORD_CATEGORIES.items():
        for word in words:
            if word in text:
                keywords.append(word)
    
    # æ•°å­—ã‚’å«ã‚€ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆé‡‘é¡ã€å¹´æ•°ãªã©ï¼‰
    numbers = re.findall(r'\d+[ä¸‡å††å¹´æœˆæ—¥æ™‚é–“äººå€‹]', text)
    keywords.extend(numbers)
    
    # ã‚«ã‚¿ã‚«ãƒŠèªï¼ˆå°‚é–€ç”¨èªã®å¯èƒ½æ€§ï¼‰
    katakana = re.findall(r'[ã‚¡-ãƒ´ãƒ¼]{3,}', text)
    keywords.extend(katakana)
    
    return list(set(keywords))

def extract_keywords_mecab(text):
    """MeCabã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º"""
    if not mecab_available:
        return extract_keywords_basic(text)
    
    keywords = []
    
    try:
        # å½¢æ…‹ç´ è§£æ
        parsed = mecab.parse(text)
        
        for line in parsed.split('\n'):
            if line == 'EOS' or line == '':
                continue
            
            parts = line.split('\t')
            if len(parts) < 2:
                continue
            
            word = parts[0]
            features = parts[1].split(',')
            
            # åè©ã‚’æŠ½å‡ºï¼ˆå›ºæœ‰åè©ã€ä¸€èˆ¬åè©ã€ã‚µå¤‰æ¥ç¶šï¼‰
            if features[0] == 'åè©' and features[1] in ['å›ºæœ‰åè©', 'ä¸€èˆ¬', 'ã‚µå¤‰æ¥ç¶š']:
                if len(word) >= 2:  # 2æ–‡å­—ä»¥ä¸Š
                    keywords.append(word)
    except:
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯åŸºæœ¬æŠ½å‡ºã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return extract_keywords_basic(text)
    
    # åŸºæœ¬æŠ½å‡ºã®çµæœã‚‚è¿½åŠ 
    keywords.extend(extract_keywords_basic(text))
    
    return list(set(keywords))

def analyze_file(filepath):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æã—ã¦ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
        title_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
        title = title_match.group(1) if title_match else os.path.basename(filepath).replace('.md', '')
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        metadata = {}
        metadata_match = re.search(r'^---\n(.*?)\n---', content, re.DOTALL)
        if metadata_match:
            metadata_text = metadata_match.group(1)
            for line in metadata_text.split('\n'):
                if ':' in line:
                    key, value = line.split(':', 1)
                    metadata[key.strip()] = value.strip()
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„éƒ¨åˆ†ã‚’æŠ½å‡º
        content_without_metadata = re.sub(r'^---\n.*?\n---\n', '', content, flags=re.DOTALL)
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        keywords = extract_keywords_mecab(content_without_metadata)
        
        # ã‚«ãƒ†ã‚´ãƒªã‚’åˆ¤å®š
        categories = []
        for category, cat_keywords in KEYWORD_CATEGORIES.items():
            if any(kw in keywords for kw in cat_keywords):
                categories.append(category)
        
        return {
            'title': title,
            'filepath': filepath,
            'keywords': keywords,
            'categories': categories,
            'metadata': metadata,
            'content_length': len(content_without_metadata),
            'has_content': len(content_without_metadata.strip()) > 100
        }
    
    except Exception as e:
        print(f"Error analyzing {filepath}: {e}")
        return None

def find_related_files(file_data, all_files_data, top_n=5):
    """é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¦‹ã¤ã‘ã‚‹"""
    if not file_data['keywords']:
        return []
    
    scores = []
    
    for other_file in all_files_data:
        if other_file['filepath'] == file_data['filepath']:
            continue
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å…±é€šåº¦ã‚’è¨ˆç®—
        common_keywords = set(file_data['keywords']) & set(other_file['keywords'])
        keyword_score = len(common_keywords)
        
        # ã‚«ãƒ†ã‚´ãƒªã®å…±é€šåº¦
        common_categories = set(file_data['categories']) & set(other_file['categories'])
        category_score = len(common_categories) * 3  # ã‚«ãƒ†ã‚´ãƒªã®é‡ã¿ã‚’é«˜ã
        
        total_score = keyword_score + category_score
        
        if total_score > 0:
            scores.append({
                'file': other_file,
                'score': total_score,
                'common_keywords': list(common_keywords),
                'common_categories': list(common_categories)
            })
    
    # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã‚’è¿”ã™
    scores.sort(key=lambda x: x['score'], reverse=True)
    return scores[:top_n]

def generate_tags(file_data):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã«é©ã—ãŸã‚¿ã‚°ã‚’ç”Ÿæˆ"""
    tags = []
    
    # ã‚«ãƒ†ã‚´ãƒªã‚¿ã‚°
    for category in file_data['categories']:
        tags.append(f"#{category.replace('ãƒ»', '_')}")
    
    # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¿ã‚°ï¼ˆé »å‡ºã™ã‚‹ã‚‚ã®ï¼‰
    keyword_counter = Counter(file_data['keywords'])
    for keyword, count in keyword_counter.most_common(5):
        if count >= 2 and len(keyword) >= 3:
            tags.append(f"#{keyword}")
    
    return list(set(tags))

def update_file_with_relations(file_data, related_files, tags):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã«é–¢é€£æƒ…å ±ã¨ã‚¿ã‚°ã‚’è¿½åŠ """
    try:
        with open(file_data['filepath'], 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ—¢å­˜ã®é–¢é€£ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤
        content = re.sub(r'\n## ğŸ”— é–¢é€£ãƒŠãƒ¬ãƒƒã‚¸\n.*?(?=\n##|\n#|$)', '', content, flags=re.DOTALL)
        content = re.sub(r'\n## ğŸ·ï¸ ã‚¿ã‚°\n.*?(?=\n##|\n#|$)', '', content, flags=re.DOTALL)
        
        # æ–°ã—ã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
        additions = []
        
        # ã‚¿ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        if tags:
            tag_section = "\n## ğŸ·ï¸ ã‚¿ã‚°\n"
            tag_section += " ".join(tags) + "\n"
            additions.append(tag_section)
        
        # é–¢é€£ãƒŠãƒ¬ãƒƒã‚¸ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        if related_files:
            relation_section = "\n## ğŸ”— é–¢é€£ãƒŠãƒ¬ãƒƒã‚¸\n"
            for rel in related_files:
                rel_file = rel['file']
                rel_title = rel_file['title']
                rel_path = os.path.relpath(rel_file['filepath'], os.path.dirname(file_data['filepath']))
                
                # Obsidianã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒªãƒ³ã‚¯
                relation_section += f"- [[{rel_title}]] - "
                
                # å…±é€šè¦ç´ ã‚’è¡¨ç¤º
                if rel['common_categories']:
                    relation_section += f"ã‚«ãƒ†ã‚´ãƒª: {', '.join(rel['common_categories'])} "
                if rel['common_keywords'][:3]:  # æœ€åˆã®3ã¤ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
                    relation_section += f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(rel['common_keywords'][:3])}"
                
                relation_section += "\n"
            
            additions.append(relation_section)
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æœ€å¾Œã«è¿½åŠ 
        if additions:
            content = content.rstrip() + "\n" + "".join(additions)
            
            with open(file_data['filepath'], 'w', encoding='utf-8') as f:
                f.write(content)
            
            return True
    
    except Exception as e:
        print(f"Error updating {file_data['filepath']}: {e}")
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("Knowledge Relation Builder")
    print("=" * 60)
    
    ensure_output_dir()
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: ã™ã¹ã¦ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ
    print("\n1. Analyzing all markdown files...")
    
    all_files_data = []
    
    # Notionãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«
    for filename in os.listdir(NOTION_DIR):
        if filename.endswith('.md') and not filename.startswith('.'):
            filepath = os.path.join(NOTION_DIR, filename)
            file_data = analyze_file(filepath)
            if file_data and file_data['has_content']:
                all_files_data.append(file_data)
    
    print(f"Analyzed {len(all_files_data)} files with content")
    
    # ã‚¹ãƒ†ãƒƒãƒ—2: é–¢é€£æ€§ã‚’è¨ˆç®—ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
    print("\n2. Building relations and updating files...")
    
    updated_count = 0
    
    for i, file_data in enumerate(all_files_data):
        print(f"\n[{i+1}/{len(all_files_data)}] {file_data['title']}")
        
        # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¦‹ã¤ã‘ã‚‹
        related_files = find_related_files(file_data, all_files_data)
        
        # ã‚¿ã‚°ã‚’ç”Ÿæˆ
        tags = generate_tags(file_data)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
        if update_file_with_relations(file_data, related_files, tags):
            updated_count += 1
            print(f"  âœ“ Added {len(tags)} tags and {len(related_files)} relations")
        else:
            print(f"  âœ— Failed to update")
    
    # ã‚¹ãƒ†ãƒƒãƒ—3: åˆ†æçµæœã‚’ä¿å­˜
    print("\n3. Saving analysis results...")
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
    category_stats = defaultdict(list)
    for file_data in all_files_data:
        for category in file_data['categories']:
            category_stats[category].append(file_data['title'])
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰çµ±è¨ˆ
    all_keywords = []
    for file_data in all_files_data:
        all_keywords.extend(file_data['keywords'])
    keyword_stats = Counter(all_keywords).most_common(50)
    
    # çµ±è¨ˆæƒ…å ±ã‚’ä¿å­˜
    stats = {
        'analysis_date': datetime.now().isoformat(),
        'total_files': len(all_files_data),
        'updated_files': updated_count,
        'category_distribution': {k: len(v) for k, v in category_stats.items()},
        'top_keywords': keyword_stats,
        'files_by_category': dict(category_stats)
    }
    
    stats_file = os.path.join(OUTPUT_DIR, 'knowledge_analysis.json')
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    print(f"\nAnalysis saved to: {stats_file}")
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print("\n" + "=" * 60)
    print("ANALYSIS SUMMARY")
    print("=" * 60)
    print(f"Total files analyzed: {len(all_files_data)}")
    print(f"Files updated: {updated_count}")
    print(f"\nCategory distribution:")
    for category, count in sorted(stats['category_distribution'].items(), key=lambda x: x[1], reverse=True):
        print(f"  - {category}: {count} files")
    print(f"\nTop 10 keywords:")
    for keyword, count in keyword_stats[:10]:
        print(f"  - {keyword}: {count} occurrences")

if __name__ == "__main__":
    main()