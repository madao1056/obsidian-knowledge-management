#!/usr/bin/env python3
"""
è‡ªå‹•è¨˜äº‹ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
ã‚¯ãƒªãƒƒãƒ—ã‹ã‚‰è¨˜äº‹ã¾ã§å®Œå…¨è‡ªå‹•åŒ–
"""

import os
import re
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple
import subprocess

class AutoArticleGenerator:
    def __init__(self, vault_path):
        self.vault_path = Path(vault_path)
        self.inbox_clip = self.vault_path / "00_Inbox" / "clip"
        self.literature = self.vault_path / "20_Literature"
        self.permanent = self.vault_path / "30_Permanent"
        self.share = self.vault_path / "70_Share" / "78_Personal"
        self.log_dir = self.vault_path / "100_cursor" / "logs"
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        self.log_dir.mkdir(exist_ok=True)
        self.share.mkdir(exist_ok=True)
        
        # ãƒ­ã‚¬ãƒ¼ã‚’è¨­å®š
        self.setup_logger()
        
    def setup_logger(self):
        """ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ ã‚’è¨­å®š"""
        log_file = self.log_dir / f"auto_article_{datetime.now().strftime('%Y%m%d')}.log"
        
        self.logger = logging.getLogger('AutoArticleGenerator')
        self.logger.setLevel(logging.INFO)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        fh = logging.FileHandler(log_file, encoding='utf-8')
        fh.setLevel(logging.INFO)
        
        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        fh.setFormatter(formatter)
        
        self.logger.addHandler(fh)
    
    def process_clip_to_literature(self):
        """Step 1: Clipã‚’Literatureã«å‡¦ç†"""
        self.logger.info("Step 1: Processing clips to Literature")
        
        # process_clip.pyã‚’å®Ÿè¡Œ
        result = subprocess.run(
            ['python3', str(self.vault_path / '100_cursor' / 'process_clip.py')],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            self.logger.info("Clips processed successfully")
            # å‡¦ç†ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆã‚’å–å¾—
            return self._get_recent_literature_files()
        else:
            self.logger.error(f"Failed to process clips: {result.stderr}")
            return []
    
    def _get_recent_literature_files(self, minutes=5):
        """æœ€è¿‘ä½œæˆã•ã‚ŒãŸLiteratureãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—"""
        recent_files = []
        cutoff_time = datetime.now().timestamp() - (minutes * 60)
        
        for file_path in self.literature.glob("**/*.md"):
            if file_path.stat().st_mtime > cutoff_time:
                recent_files.append(file_path)
        
        return recent_files
    
    def extract_key_insights(self, file_path):
        """Step 2: Literatureã‹ã‚‰é‡è¦ãªæ´å¯Ÿã‚’æŠ½å‡º"""
        self.logger.info(f"Step 2: Extracting insights from {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        metadata = self._extract_metadata_from_content(content)
        
        # ä¸»è¦ãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡º
        main_points = self._extract_main_points(content)
        
        # æ´»ç”¨ä¾‹ã‚’æŠ½å‡º
        use_cases = self._extract_use_cases(content)
        
        # æ´å¯Ÿã‚’ç”Ÿæˆï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯AI APIã‚’ä½¿ç”¨æ¨å¥¨ï¼‰
        insights = self._generate_insights(content, main_points, use_cases)
        
        return {
            'file_path': file_path,
            'metadata': metadata,
            'main_points': main_points,
            'use_cases': use_cases,
            'insights': insights,
            'original_content': content
        }
    
    def _extract_metadata_from_content(self, content):
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        metadata = {}
        
        # å‡¦ç†æ¸ˆã¿ã®Literatureãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰æŠ½å‡º
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
        title_match = re.search(r'^# (.+)$', content, re.MULTILINE)
        if title_match:
            metadata['title'] = title_match.group(1)
        
        # ã‚½ãƒ¼ã‚¹ã‚’æŠ½å‡º
        source_match = re.search(r'- \*\*ã‚½ãƒ¼ã‚¹\*\*: (.+)$', content, re.MULTILINE)
        if source_match:
            metadata['source'] = source_match.group(1)
        
        # ã‚ªãƒ¼ãƒŠãƒ¼ã‚’æŠ½å‡º
        owner_match = re.search(r'- \*\*ã‚ªãƒ¼ãƒŠãƒ¼\*\*: (.+)$', content, re.MULTILINE)
        if owner_match:
            metadata['owner'] = owner_match.group(1)
        
        # æ—¥ä»˜ã‚’æŠ½å‡º
        date_match = re.search(r'- \*\*æ—¥ä»˜\*\*: (.+)$', content, re.MULTILINE)
        if date_match:
            metadata['date'] = date_match.group(1)
        
        # ã‚¿ã‚°ã‚’æŠ½å‡º
        tags = re.findall(r'#[\w/]+', content)
        metadata['tags'] = list(set(tags))
        
        # è¨˜äº‹ã®æ¦‚è¦ã‚’æŠ½å‡º
        summary_match = re.search(r'## è¨˜äº‹ã®æ¦‚è¦\n(.+?)(?=\n##|\Z)', content, re.DOTALL)
        if summary_match:
            metadata['summary'] = summary_match.group(1).strip()
        
        return metadata
    
    def _extract_main_points(self, content):
        """ä¸»è¦ãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡º"""
        points = []
        
        # ä¸»è¦ãƒã‚¤ãƒ³ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã™
        section_match = re.search(r'## ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ\n(.*?)(?=##|\Z)', content, re.DOTALL)
        if section_match:
            section_content = section_match.group(1)
            # ç®‡æ¡æ›¸ãã‚’æŠ½å‡º
            points = re.findall(r'^- (.+)$', section_content, re.MULTILINE)
        
        return points
    
    def _extract_use_cases(self, content):
        """æ´»ç”¨ä¾‹ã‚’æŠ½å‡º"""
        use_cases = []
        
        # æ´»ç”¨ä¾‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã™
        section_match = re.search(r'## æ´»ç”¨ä¾‹\n(.*?)(?=##|\Z)', content, re.DOTALL)
        if section_match:
            section_content = section_match.group(1)
            # ç®‡æ¡æ›¸ãã‚’æŠ½å‡º
            use_cases = re.findall(r'^- (.+)$', section_content, re.MULTILINE)
        
        return use_cases
    
    def _generate_insights(self, content, main_points, use_cases):
        """æ´å¯Ÿã‚’ç”Ÿæˆï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        insights = []
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“åˆ†æ
        keywords = {
            'åŠ¹ç‡': 'ç”Ÿç”£æ€§å‘ä¸Šã®è¦³ç‚¹ã‹ã‚‰é‡è¦',
            'AI': 'äººå·¥çŸ¥èƒ½ã®æ´»ç”¨ã«ã‚ˆã‚‹é©æ–°',
            'è‡ªå‹•åŒ–': 'ãƒ—ãƒ­ã‚»ã‚¹ã®åŠ¹ç‡åŒ–ã«è²¢çŒ®',
            'å­¦ç¿’': 'ç¶™ç¶šçš„ãªæˆé•·ã®æ©Ÿä¼š',
            'ãƒ‡ãƒ¼ã‚¿': 'ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³ãªæ„æ€æ±ºå®š',
            'ã‚·ã‚¹ãƒ†ãƒ ': 'ã‚·ã‚¹ãƒ†ãƒ æ€è€ƒã®é©ç”¨',
            'ãƒ„ãƒ¼ãƒ«': 'é©åˆ‡ãªãƒ„ãƒ¼ãƒ«é¸æŠã®é‡è¦æ€§'
        }
        
        content_lower = content.lower()
        for keyword, insight_template in keywords.items():
            if keyword.lower() in content_lower:
                insights.append(f"{insight_template}ãŒç¤ºå”†ã•ã‚Œã‚‹")
        
        # ä¸»è¦ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰æ´å¯Ÿã‚’ç”Ÿæˆ
        if len(main_points) > 3:
            insights.append("å¤šé¢çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå¿…è¦ãªè¤‡é›‘ãªãƒˆãƒ”ãƒƒã‚¯")
        
        # æ´»ç”¨ä¾‹ã‹ã‚‰æ´å¯Ÿã‚’ç”Ÿæˆ
        if use_cases:
            insights.append(f"{len(use_cases)}ã¤ã®å…·ä½“çš„ãªæ´»ç”¨æ–¹æ³•ãŒå­˜åœ¨")
        
        return insights[:3]  # ä¸Šä½3ã¤ã®æ´å¯Ÿ
    
    def create_permanent_note(self, literature_data):
        """Step 3: Permanentãƒãƒ¼ãƒˆã‚’ä½œæˆ"""
        self.logger.info("Step 3: Creating Permanent note")
        
        # ã‚«ãƒ†ã‚´ãƒªã‚’æ±ºå®š
        category = self._determine_category(literature_data)
        
        # Permanentãƒãƒ¼ãƒˆã®å†…å®¹ã‚’ç”Ÿæˆ
        permanent_content = self._generate_permanent_content(literature_data)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        title_clean = re.sub(r'[^\w\s-]', '', literature_data['metadata'].get('title', 'Untitled'))[:30]
        filename = f"{title_clean}_æ´å¯Ÿ.md"
        file_path = self.permanent / category / filename
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        file_path.parent.mkdir(exist_ok=True)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(permanent_content)
        
        self.logger.info(f"Permanent note created: {file_path}")
        
        return file_path, permanent_content
    
    def _determine_category(self, literature_data):
        """ã‚«ãƒ†ã‚´ãƒªã‚’è‡ªå‹•åˆ¤å®š"""
        content = literature_data['original_content'].lower()
        tags = literature_data['metadata'].get('tags', [])
        
        # ã‚¿ã‚°ãƒ™ãƒ¼ã‚¹ã®åˆ¤å®š
        if any('tech' in tag for tag in tags):
            return '32_Tech'
        elif any('philosophy' in tag or 'å“²å­¦' in tag for tag in tags):
            return '31_Philosophy'
        elif any('productivity' in tag or 'ç”Ÿç”£æ€§' in tag for tag in tags):
            return '33_Productivity'
        elif any('ai' in tag for tag in tags):
            return '35_AI'
        elif any('product' in tag for tag in tags):
            return '34_Product'
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®åˆ¤å®š
        if 'ãƒ—ãƒ­ã‚°ãƒ©' in content or 'ã‚³ãƒ¼ãƒ‰' in content or 'é–‹ç™º' in content:
            return '32_Tech'
        elif 'ç”Ÿç”£æ€§' in content or 'åŠ¹ç‡' in content:
            return '33_Productivity'
        elif 'ai' in content or 'äººå·¥çŸ¥èƒ½' in content:
            return '35_AI'
        else:
            return '32_Tech'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def _generate_permanent_content(self, literature_data):
        """Permanentãƒãƒ¼ãƒˆã®å†…å®¹ã‚’ç”Ÿæˆ"""
        metadata = literature_data['metadata']
        insights = literature_data['insights']
        main_points = literature_data['main_points']
        
        content = f"""# {metadata.get('title', 'Untitled')}ã®æ´å¯Ÿ

## æ¦‚è¦
{metadata.get('title', '')}ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸçŸ¥è­˜ã‚’è‡ªåˆ†ã®è¨€è‘‰ã§å†æ§‹ç¯‰ã—ãŸãƒãƒ¼ãƒˆã€‚

## æ ¸å¿ƒæ¦‚å¿µ

### ä¸»è¦ãªæ´å¯Ÿ
"""
        
        for insight in insights:
            content += f"- {insight}\n"
        
        content += """

### å®Ÿè·µã¸ã®å¿œç”¨
"""
        
        for i, point in enumerate(main_points[:3], 1):
            content += f"{i}. {point}\n"
        
        content += f"""

## é–¢é€£ã™ã‚‹æ¦‚å¿µ
- [[{metadata.get('title', 'Original')}]] - å…ƒã®Literatureãƒãƒ¼ãƒˆ

## ä»Šå¾Œã®å±•é–‹
ã“ã®æ´å¯Ÿã‚’åŸºã«ã€å®Ÿè·µçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¤œè¨ã—ã€å…·ä½“çš„ãªè¡Œå‹•è¨ˆç”»ã‚’ç«‹ã¦ã‚‹ã€‚

---

**ä½œæˆæ—¥**: {datetime.now().strftime('%Y-%m-%d')}  
**ã‚¿ã‚°**: #permanent {' '.join(metadata.get('tags', []))}  
**å‚è€ƒæ–‡çŒ®**: [[{Path(literature_data['file_path']).stem}]]
"""
        
        return content
    
    def create_article(self, permanent_data, literature_data):
        """Step 4: è¨˜äº‹ã‚’ç”Ÿæˆ"""
        self.logger.info("Step 4: Creating article")
        
        # è¨˜äº‹ã®å†…å®¹ã‚’ç”Ÿæˆ
        article_content = self._generate_article_content(permanent_data, literature_data)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        date_prefix = datetime.now().strftime('%Y%m%d')
        title = literature_data['metadata'].get('title', 'Untitled')
        title_clean = re.sub(r'[^\w\s-]', '', title)[:30]
        filename = f"{date_prefix}_{title_clean}_è¨˜äº‹.md"
        file_path = self.share / filename
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(article_content)
        
        self.logger.info(f"Article created: {file_path}")
        
        return file_path
    
    def _generate_article_content(self, permanent_data, literature_data):
        """è¨˜äº‹ã®å†…å®¹ã‚’ç”Ÿæˆ"""
        metadata = literature_data['metadata']
        insights = literature_data['insights']
        main_points = literature_data['main_points']
        use_cases = literature_data['use_cases']
        
        # è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
        title = f"{metadata.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«')}ã‹ã‚‰å­¦ã¶å®Ÿè·µçš„ãªçŸ¥è¦‹"
        
        content = f"""# {title}

## ã¯ã˜ã‚ã«

æœ€è¿‘ã€{metadata.get('title', 'ã‚ã‚‹è¨˜äº‹')}ã‚’èª­ã‚“ã§ã€ã„ãã¤ã‹ã®é‡è¦ãªæ°—ã¥ãã‚’å¾—ã¾ã—ãŸã€‚
ã“ã®è¨˜äº‹ã§ã¯ã€ãã®å†…å®¹ã‚’æ•´ç†ã—ã€å®Ÿè·µçš„ãªè¦³ç‚¹ã‹ã‚‰è€ƒå¯Ÿã—ã¦ã¿ãŸã„ã¨æ€ã„ã¾ã™ã€‚

## è¨˜äº‹ã®æ¦‚è¦

å…ƒã®è¨˜äº‹ã§ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªç‚¹ãŒè­°è«–ã•ã‚Œã¦ã„ã¾ã—ãŸï¼š

"""
        
        # ä¸»è¦ãƒã‚¤ãƒ³ãƒˆã‚’è¨˜è¼‰
        for point in main_points[:3]:
            content += f"- {point}\n"
        
        content += """

## é‡è¦ãªæ´å¯Ÿ

### 1. å®Ÿè·µçš„ãªè¦–ç‚¹

"""
        
        if insights:
            content += f"{insights[0]}ã“ã¨ãŒã€ã“ã®è¨˜äº‹ã‹ã‚‰èª­ã¿å–ã‚Œã¾ã™ã€‚"
        
        content += """

### 2. å¿œç”¨ã®å¯èƒ½æ€§

ä»¥ä¸‹ã®ã‚ˆã†ãªå ´é¢ã§æ´»ç”¨ã§ãã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ï¼š

"""
        
        for use_case in use_cases[:3]:
            content += f"- {use_case}\n"
        
        content += """

## ç­†è€…ã®æ„Ÿæƒ³

å€‹äººçš„ã«æœ€ã‚‚å°è±¡çš„ã ã£ãŸã®ã¯ã€"""
        
        if main_points:
            content += f"ã€Œ{main_points[0]}ã€ã¨ã„ã†ç‚¹ã§ã™ã€‚"
        
        content += """
ã“ã‚Œã¯æ—¥å¸¸ã®æ¥­å‹™ã‚„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ãŠã„ã¦ã‚‚ã€é‡è¦ãªç¤ºå”†ã‚’ä¸ãˆã¦ãã‚Œã¾ã™ã€‚

## è¨˜äº‹ã®ãƒã‚¤ãƒ³ãƒˆ

1. **çŸ¥è­˜ã®ä½“ç³»åŒ–**: æ–­ç‰‡çš„ãªæƒ…å ±ã‚’æ§‹é€ åŒ–ã™ã‚‹ã“ã¨ã®é‡è¦æ€§
2. **å®Ÿè·µã¸ã®å¿œç”¨**: ç†è«–ã‚’å…·ä½“çš„ãªè¡Œå‹•ã«è½ã¨ã—è¾¼ã‚€æ–¹æ³•
3. **ç¶™ç¶šçš„ãªå­¦ç¿’**: æ–°ã—ã„çŸ¥è­˜ã‚’æ—¢å­˜ã®çŸ¥è­˜ã¨çµã³ã¤ã‘ã‚‹

## ã¾ã¨ã‚

"""
        
        content += f"""{metadata.get('title', 'ã“ã®è¨˜äº‹')}ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸçŸ¥è¦‹ã¯ã€å˜ãªã‚‹æƒ…å ±ä»¥ä¸Šã®ä¾¡å€¤ãŒã‚ã‚Šã¾ã™ã€‚
é‡è¦ãªã®ã¯ã€ã“ã‚Œã‚‰ã®çŸ¥è­˜ã‚’ã©ã®ã‚ˆã†ã«è‡ªåˆ†ã®æ–‡è„ˆã§æ´»ç”¨ã™ã‚‹ã‹ã¨ã„ã†ã“ã¨ã§ã™ã€‚

ä»Šå¾Œã‚‚ç¶™ç¶šçš„ã«å­¦ç¿’ã‚’ç¶šã‘ã€å®Ÿè·µçš„ãªçŸ¥è­˜ã¨ã—ã¦æ˜‡è¯ã•ã›ã¦ã„ããŸã„ã¨æ€ã„ã¾ã™ã€‚

---

**å…¬é–‹æ—¥**: {datetime.now().strftime('%Y-%m-%d')}  
**ã‚«ãƒ†ã‚´ãƒª**: å­¦ç¿’ãƒãƒ¼ãƒˆ  
**ã‚¿ã‚°**: #share/blog {' '.join(metadata.get('tags', []))}  
**å‚è€ƒè³‡æ–™**: 
- [[{Path(literature_data['file_path']).stem}]]
- {metadata.get('source', '')}
"""
        
        return content
    
    def process_all_clips(self):
        """ã™ã¹ã¦ã®ã‚¯ãƒªãƒƒãƒ—ã‚’å‡¦ç†ã—ã¦è¨˜äº‹ã¾ã§ç”Ÿæˆ"""
        self.logger.info("Starting full automation process")
        print("ğŸš€ Starting full automation: Clip â†’ Literature â†’ Permanent â†’ Article")
        
        # Step 1: Clipã‚’Literatureã«å‡¦ç†
        literature_files = self.process_clip_to_literature()
        
        if not literature_files:
            print("No clips to process or processing failed")
            return []
        
        generated_articles = []
        
        for lit_file in literature_files:
            try:
                print(f"\nğŸ“„ Processing: {lit_file.name}")
                
                # Step 2: æ´å¯Ÿã‚’æŠ½å‡º
                literature_data = self.extract_key_insights(lit_file)
                
                # Step 3: Permanentãƒãƒ¼ãƒˆä½œæˆ
                permanent_path, permanent_content = self.create_permanent_note(literature_data)
                print(f"  âœ“ Permanent note created: {permanent_path.name}")
                
                # Step 4: è¨˜äº‹ç”Ÿæˆ
                article_path = self.create_article(
                    {'path': permanent_path, 'content': permanent_content},
                    literature_data
                )
                print(f"  âœ“ Article created: {article_path.name}")
                
                generated_articles.append({
                    'literature': lit_file,
                    'permanent': permanent_path,
                    'article': article_path,
                    'metadata': literature_data['metadata']
                })
                
            except Exception as e:
                self.logger.error(f"Error processing {lit_file}: {str(e)}")
                print(f"  âœ— Error: {str(e)}")
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
        self._generate_automation_report(generated_articles)
        
        return generated_articles
    
    def _generate_automation_report(self, generated_articles):
        """è‡ªå‹•åŒ–å‡¦ç†ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        report_path = self.vault_path / "100_cursor" / "reports" / f"automation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        report_path.parent.mkdir(exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"# Automation Report\n\n")
            f.write(f"**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**Articles Generated**: {len(generated_articles)}\n\n")
            
            if generated_articles:
                f.write("## Generated Articles\n\n")
                for item in generated_articles:
                    f.write(f"### {item['metadata'].get('title', 'Untitled')}\n")
                    f.write(f"- Literature: `{item['literature'].name}`\n")
                    f.write(f"- Permanent: `{item['permanent'].name}`\n")
                    f.write(f"- Article: `{item['article'].name}`\n")
                    f.write(f"- Source: {item['metadata'].get('source', 'Unknown')}\n\n")
        
        print(f"\nğŸ“Š Report generated: {report_path.name}")
        print(f"âœ… Total articles generated: {len(generated_articles)}")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    vault_path = "/Users/hashiguchimasaki/project/obsidian"
    generator = AutoArticleGenerator(vault_path)
    
    # ã™ã¹ã¦ã®ã‚¯ãƒªãƒƒãƒ—ã‚’å‡¦ç†
    results = generator.process_all_clips()
    
    if results:
        print("\nğŸ‰ Automation complete!")
        print(f"Generated {len(results)} articles from clips")
    else:
        print("\nğŸ“­ No clips found to process")


if __name__ == "__main__":
    main()