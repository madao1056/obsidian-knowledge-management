#!/usr/bin/env python3
"""
Notionã‹ã‚‰å–å¾—ã—ãŸã™ã¹ã¦ã®çŸ¥è­˜ã‚’çµ±åˆã—ã€æœ€é©ãªçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import os
import re
import json
import shutil
from datetime import datetime
from pathlib import Path
import hashlib

# è¨­å®š
BASE_DIR = "/Users/hashiguchimasaki/project/obsidian"
NOTION_DIR = os.path.join(BASE_DIR, "20_Literature/25_Notion")
KNOWLEDGE_BASE_DIR = os.path.join(BASE_DIR, "10_Projects/Knowledge_Base")
BACKUP_DIR = os.path.join(BASE_DIR, "99_Archive/Notion_Backup_" + datetime.now().strftime("%Y%m%d"))

# ã‚«ãƒ†ã‚´ãƒªå®šç¾©
CATEGORIES = {
    "ã‚ˆã—ãªã«å¯¾å¿œ": {
        "keywords": ["ã‚ˆã—ãªã«", "å¯¾å¿œ", "å·»ãå–", "å…ˆå›ã‚Š", "ç›¸æ‰‹ç›®ç·š", "ææ¡ˆ", "è³ªå•åŠ›"],
        "dir": "01_ã‚ˆã—ãªã«å¯¾å¿œ",
        "priority": 1
    },
    "Webåˆ¶ä½œ": {
        "keywords": ["WordPress", "HTML", "CSS", "JavaScript", "PHP", "ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°", "ãƒ‡ã‚¶ã‚¤ãƒ³", "å®Ÿè£…"],
        "dir": "02_Webåˆ¶ä½œ",
        "priority": 2
    },
    "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°": {
        "keywords": ["ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°", "é›†å®¢", "SNS", "SEO", "åºƒå‘Š", "åˆ†æ", "æˆ¦ç•¥"],
        "dir": "03_ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°",
        "priority": 3
    },
    "ãƒ“ã‚¸ãƒã‚¹": {
        "keywords": ["ãƒ“ã‚¸ãƒã‚¹", "çµŒå–¶", "å˜ä¾¡", "å–¶æ¥­", "å¥‘ç´„", "æ¡ˆä»¶", "ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"],
        "dir": "04_ãƒ“ã‚¸ãƒã‚¹",
        "priority": 4
    },
    "å­¦ç¿’ãƒ»æˆé•·": {
        "keywords": ["å­¦ç¿’", "æˆé•·", "ã‚¹ã‚­ãƒ«", "å‹‰å¼·", "ç¿’æ…£", "æ”¹å–„", "PDCA"],
        "dir": "05_å­¦ç¿’ãƒ»æˆé•·",
        "priority": 5
    },
    "ã‚°ãƒƒã‚µãƒãƒ»ãƒ©ãƒœ": {
        "keywords": ["ã‚°ãƒƒã‚µãƒ", "ãƒ©ãƒœ", "ã‚µãƒãƒ¼ãƒˆ", "ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£", "Discord"],
        "dir": "06_ã‚°ãƒƒã‚µãƒãƒ»ãƒ©ãƒœ",
        "priority": 6
    },
    "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ": {
        "keywords": ["ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ", "æ¡ˆä»¶", "åˆ¶ä½œ", "ç´å“", "ä¿®æ­£", "é€²æ—"],
        "dir": "07_ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ",
        "priority": 7
    },
    "ãã®ä»–": {
        "keywords": [],
        "dir": "99_ãã®ä»–",
        "priority": 99
    }
}

class KnowledgeIntegrator:
    def __init__(self):
        self.file_index = {}
        self.content_hashes = {}
        self.relations = {}
        self.tags = {}
        
    def create_directories(self):
        """å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä½œæˆ"""
        print("Creating directory structure...")
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        os.makedirs(BACKUP_DIR, exist_ok=True)
        
        # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        os.makedirs(KNOWLEDGE_BASE_DIR, exist_ok=True)
        
        # ã‚«ãƒ†ã‚´ãƒªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        for category, info in CATEGORIES.items():
            category_dir = os.path.join(KNOWLEDGE_BASE_DIR, info["dir"])
            os.makedirs(category_dir, exist_ok=True)
        
        # ç‰¹åˆ¥ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        os.makedirs(os.path.join(KNOWLEDGE_BASE_DIR, "00_Index"), exist_ok=True)
        os.makedirs(os.path.join(KNOWLEDGE_BASE_DIR, "00_Templates"), exist_ok=True)
        
    def backup_existing_files(self):
        """æ—¢å­˜ã®Notionãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
        print("Backing up existing files...")
        
        if os.path.exists(NOTION_DIR):
            # Notionãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå…¨ä½“ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            backup_path = os.path.join(BACKUP_DIR, "25_Notion")
            shutil.copytree(NOTION_DIR, backup_path, dirs_exist_ok=True)
            print(f"  Backed up to: {backup_path}")
    
    def scan_all_files(self):
        """ã™ã¹ã¦ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³"""
        print("\nScanning all markdown files...")
        
        file_count = 0
        for root, dirs, files in os.walk(NOTION_DIR):
            for filename in files:
                if filename.endswith('.md') and not filename.startswith('.'):
                    filepath = os.path.join(root, filename)
                    relative_path = os.path.relpath(filepath, NOTION_DIR)
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’èª­ã¿è¾¼ã¿
                    try:
                        with open(filepath, 'r', encoding='utf-8') as f:
                            content = f.read()
                        
                        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰
                        content_hash = hashlib.md5(content.encode()).hexdigest()
                        
                        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                        metadata = self.extract_metadata(content)
                        
                        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ 
                        self.file_index[filepath] = {
                            "relative_path": relative_path,
                            "filename": filename,
                            "content_hash": content_hash,
                            "metadata": metadata,
                            "category": None,
                            "tags": [],
                            "relations": []
                        }
                        
                        file_count += 1
                        
                    except Exception as e:
                        print(f"  Error reading {filepath}: {e}")
        
        print(f"  Found {file_count} files")
        return file_count
    
    def extract_metadata(self, content):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        metadata = {}
        
        # YAMLãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ã‚’æŠ½å‡º
        yaml_match = re.search(r'^---\n(.*?)\n---', content, re.DOTALL)
        if yaml_match:
            yaml_content = yaml_match.group(1)
            
            # ç°¡æ˜“çš„ãªYAMLãƒ‘ãƒ¼ã‚¹
            for line in yaml_content.split('\n'):
                if ':' in line:
                    key, value = line.split(':', 1)
                    metadata[key.strip()] = value.strip()
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
        title_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
        if title_match:
            metadata['title'] = title_match.group(1).strip()
        
        # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‹ã©ã†ã‹
        metadata['is_placeholder'] = 'placeholder file from Notion' in content
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®é•·ã•
        metadata['content_length'] = len(content)
        
        return metadata
    
    def categorize_files(self):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡"""
        print("\nCategorizing files...")
        
        for filepath, file_info in self.file_index.items():
            filename = file_info['filename']
            content = self.read_file_content(filepath)
            
            # æœ€ã‚‚é©åˆ‡ãªã‚«ãƒ†ã‚´ãƒªã‚’æ±ºå®š
            best_category = "ãã®ä»–"
            best_score = 0
            
            for category, cat_info in CATEGORIES.items():
                if category == "ãã®ä»–":
                    continue
                
                score = 0
                for keyword in cat_info["keywords"]:
                    if keyword.lower() in filename.lower():
                        score += 10
                    if keyword.lower() in content.lower():
                        score += 1
                
                # å„ªå…ˆåº¦ã‚’è€ƒæ…®
                score = score / cat_info["priority"]
                
                if score > best_score:
                    best_score = score
                    best_category = category
            
            file_info['category'] = best_category
            file_info['score'] = best_score
    
    def read_file_content(self, filepath):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’èª­ã¿è¾¼ã‚€"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                return f.read()
        except:
            return ""
    
    def extract_tags_and_relations(self):
        """ã‚¿ã‚°ã¨é–¢é€£æ€§ã‚’æŠ½å‡º"""
        print("\nExtracting tags and relations...")
        
        # ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        all_keywords = set()
        
        for filepath, file_info in self.file_index.items():
            content = self.read_file_content(filepath)
            
            # ã‚¿ã‚°ã‚’æŠ½å‡º
            tags = []
            
            # #ã‚¿ã‚°å½¢å¼
            tag_matches = re.findall(r'#([^\s#]+)', content)
            tags.extend(tag_matches)
            
            # ã‚«ãƒ†ã‚´ãƒªã‚’ã‚¿ã‚°ã«è¿½åŠ 
            if file_info['category']:
                tags.append(file_info['category'])
            
            # ã‚ˆã—ãªã«é–¢é€£ã®ç‰¹åˆ¥ã‚¿ã‚°
            if 'ã‚ˆã—ãªã«' in content:
                tags.append('ã‚ˆã—ãªã«å¯¾å¿œ')
            
            file_info['tags'] = list(set(tags))
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åé›†
            words = re.findall(r'[ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥ãƒ¼]+|[a-zA-Z]+', content)
            all_keywords.update(words)
        
        # é–¢é€£æ€§ã‚’è¨ˆç®—
        self.calculate_relations(all_keywords)
    
    def calculate_relations(self, all_keywords):
        """ãƒ•ã‚¡ã‚¤ãƒ«é–“ã®é–¢é€£æ€§ã‚’è¨ˆç®—"""
        print("  Calculating file relations...")
        
        # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å®šç¾©
        important_keywords = {
            'ã‚ˆã—ãªã«', 'å¯¾å¿œ', 'ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ', 'ææ¡ˆ', 'è³ªå•',
            'WordPress', 'ãƒ‡ã‚¶ã‚¤ãƒ³', 'ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°', 'å®Ÿè£…',
            'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°', 'é›†å®¢', 'å˜ä¾¡', 'æ¡ˆä»¶'
        }
        
        file_paths = list(self.file_index.keys())
        
        for i, filepath1 in enumerate(file_paths):
            content1 = self.read_file_content(filepath1)
            relations = []
            
            for j, filepath2 in enumerate(file_paths):
                if i >= j:  # è‡ªåˆ†è‡ªèº«ã¨æ—¢ã«è¨ˆç®—æ¸ˆã¿ã®ãƒšã‚¢ã¯ã‚¹ã‚­ãƒƒãƒ—
                    continue
                
                content2 = self.read_file_content(filepath2)
                
                # é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                score = 0
                
                # å…±é€šã®é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
                for keyword in important_keywords:
                    if keyword in content1 and keyword in content2:
                        score += 10
                
                # ç›¸äº’å‚ç…§
                filename1 = self.file_index[filepath1]['filename']
                filename2 = self.file_index[filepath2]['filename']
                
                if filename2.replace('.md', '') in content1:
                    score += 20
                if filename1.replace('.md', '') in content2:
                    score += 20
                
                # åŒã˜ã‚«ãƒ†ã‚´ãƒª
                if self.file_index[filepath1]['category'] == self.file_index[filepath2]['category']:
                    score += 5
                
                if score >= 15:  # é–¾å€¤ä»¥ä¸Šãªã‚‰é–¢é€£ã‚ã‚Šã¨ã™ã‚‹
                    relations.append({
                        'file': filepath2,
                        'score': score
                    })
            
            # ã‚¹ã‚³ã‚¢ã®é«˜ã„é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã‚’ä¿å­˜
            relations.sort(key=lambda x: x['score'], reverse=True)
            self.file_index[filepath1]['relations'] = relations[:10]  # ä¸Šä½10ä»¶
    
    def remove_duplicates(self):
        """é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†"""
        print("\nProcessing duplicate files...")
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒƒã‚·ãƒ¥ã”ã¨ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        hash_groups = {}
        for filepath, file_info in self.file_index.items():
            content_hash = file_info['content_hash']
            if content_hash not in hash_groups:
                hash_groups[content_hash] = []
            hash_groups[content_hash].append(filepath)
        
        # é‡è¤‡ã‚’å‡¦ç†
        duplicates_removed = 0
        for content_hash, filepaths in hash_groups.items():
            if len(filepaths) > 1:
                # æœ€ã‚‚æƒ…å ±é‡ã®å¤šã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿æŒ
                best_file = max(filepaths, key=lambda f: self.file_index[f]['metadata'].get('content_length', 0))
                
                for filepath in filepaths:
                    if filepath != best_file:
                        self.file_index[filepath]['duplicate_of'] = best_file
                        duplicates_removed += 1
        
        print(f"  Found {duplicates_removed} duplicate files")
        return duplicates_removed
    
    def reorganize_files(self):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–°ã—ã„æ§‹é€ ã«å†ç·¨æˆ"""
        print("\nReorganizing files...")
        
        files_moved = 0
        
        for filepath, file_info in self.file_index.items():
            # é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—
            if 'duplicate_of' in file_info:
                continue
            
            # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—
            if file_info['metadata'].get('is_placeholder', False):
                if file_info['metadata'].get('content_length', 0) < 1000:
                    continue
            
            # æ–°ã—ã„ãƒ‘ã‚¹ã‚’æ±ºå®š
            category = file_info['category']
            category_info = CATEGORIES[category]
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            clean_filename = self.clean_filename(file_info['filename'])
            
            # æ–°ã—ã„ãƒ‘ã‚¹
            new_dir = os.path.join(KNOWLEDGE_BASE_DIR, category_info['dir'])
            new_path = os.path.join(new_dir, clean_filename)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼ï¼ˆæ—¢å­˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ®‹ã™ï¼‰
            try:
                os.makedirs(os.path.dirname(new_path), exist_ok=True)
                shutil.copy2(filepath, new_path)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®ãƒªãƒ³ã‚¯ã‚’æ›´æ–°
                self.update_file_links(new_path, filepath, new_path)
                
                files_moved += 1
                
            except Exception as e:
                print(f"  Error moving {filepath}: {e}")
        
        print(f"  Reorganized {files_moved} files")
        return files_moved
    
    def clean_filename(self, filename):
        """ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        # ä¸è¦ãªæ–‡å­—ã‚’å‰Šé™¤
        filename = re.sub(r'[<>:"/\\|?*]', '', filename)
        
        # é•·ã™ãã‚‹å ´åˆã¯çŸ­ç¸®
        if len(filename) > 100:
            name, ext = os.path.splitext(filename)
            filename = name[:97] + '...' + ext
        
        return filename
    
    def update_file_links(self, filepath, old_path, new_path):
        """ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®ãƒªãƒ³ã‚¯ã‚’æ›´æ–°"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒªãƒ³ã‚¯ã‚’è¿½åŠ 
            file_info = self.file_index.get(old_path, {})
            relations = file_info.get('relations', [])
            
            if relations:
                # é–¢é€£ãƒªãƒ³ã‚¯ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
                related_section = "\n\n## é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ\n\n"
                
                for relation in relations[:5]:  # ä¸Šä½5ä»¶
                    related_file = relation['file']
                    related_info = self.file_index.get(related_file, {})
                    related_title = related_info.get('metadata', {}).get('title', os.path.basename(related_file))
                    
                    # ç›¸å¯¾ãƒ‘ã‚¹ã‚’è¨ˆç®—
                    related_category = related_info.get('category', 'ãã®ä»–')
                    related_filename = self.clean_filename(os.path.basename(related_file))
                    related_path = f"../{CATEGORIES[related_category]['dir']}/{related_filename}"
                    
                    related_section += f"- [[{related_path}|{related_title}]]\n"
                
                # ã‚¿ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
                tags = file_info.get('tags', [])
                if tags:
                    tag_section = "\n\n## ã‚¿ã‚°\n\n"
                    for tag in tags:
                        tag_section += f"#{tag} "
                    content += tag_section
                
                content += related_section
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(content)
                    
        except Exception as e:
            print(f"  Error updating links in {filepath}: {e}")
    
    def create_index_files(self):
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
        print("\nCreating index files...")
        
        # ãƒ¡ã‚¤ãƒ³ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        self.create_main_index()
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        self.create_category_indexes()
        
        # ã‚ˆã—ãªã«å¯¾å¿œç‰¹åˆ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        self.create_yoshinani_index()
    
    def create_main_index(self):
        """ãƒ¡ã‚¤ãƒ³ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
        index_path = os.path.join(KNOWLEDGE_BASE_DIR, "00_Index", "README.md")
        
        content = """# ğŸ“š ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

ã“ã®ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã¯ã€Notionã‹ã‚‰çµ±åˆã•ã‚ŒãŸã™ã¹ã¦ã®çŸ¥è­˜ã‚’ä½“ç³»çš„ã«æ•´ç†ã—ãŸã‚‚ã®ã§ã™ã€‚

## ğŸ—‚ï¸ ã‚«ãƒ†ã‚´ãƒª

"""
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
        category_stats = {}
        for file_info in self.file_index.values():
            if 'duplicate_of' not in file_info:
                category = file_info['category']
                category_stats[category] = category_stats.get(category, 0) + 1
        
        # ã‚«ãƒ†ã‚´ãƒªãƒªã‚¹ãƒˆ
        for category, info in sorted(CATEGORIES.items(), key=lambda x: x[1]['priority']):
            count = category_stats.get(category, 0)
            if count > 0:
                content += f"### [[../{info['dir']}/README.md|{category}]] ({count}ä»¶)\n"
                content += f"{', '.join(info['keywords'][:5])}\n\n"
        
        content += """
## ğŸ” ã‚¯ã‚¤ãƒƒã‚¯ã‚¢ã‚¯ã‚»ã‚¹

- [[../01_ã‚ˆã—ãªã«å¯¾å¿œ/ã‚ˆã—ãªã«å¯¾å¿œ_å®Œå…¨ã‚¬ã‚¤ãƒ‰.md|ã‚ˆã—ãªã«å¯¾å¿œ å®Œå…¨ã‚¬ã‚¤ãƒ‰]]
- [[../02_Webåˆ¶ä½œ/WordPress_ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹.md|WordPress ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹]]
- [[../04_ãƒ“ã‚¸ãƒã‚¹/å˜ä¾¡UPæˆ¦ç•¥.md|å˜ä¾¡UPæˆ¦ç•¥]]

## ğŸ“Š çµ±è¨ˆæƒ…å ±

"""
        
        # çµ±è¨ˆæƒ…å ±
        total_files = len([f for f in self.file_index.values() if 'duplicate_of' not in f])
        total_duplicates = len([f for f in self.file_index.values() if 'duplicate_of' in f])
        
        content += f"- ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_files}\n"
        content += f"- é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«: {total_duplicates}\n"
        content += f"- ã‚«ãƒ†ã‚´ãƒªæ•°: {len([c for c in category_stats.values() if c > 0])}\n"
        content += f"\næ›´æ–°æ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        
        with open(index_path, 'w', encoding='utf-8') as f:
            f.write(content)
    
    def create_category_indexes(self):
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ"""
        for category, info in CATEGORIES.items():
            category_dir = os.path.join(KNOWLEDGE_BASE_DIR, info['dir'])
            index_path = os.path.join(category_dir, "README.md")
            
            # ã‚«ãƒ†ã‚´ãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†
            category_files = []
            for filepath, file_info in self.file_index.items():
                if file_info['category'] == category and 'duplicate_of' not in file_info:
                    category_files.append((filepath, file_info))
            
            if not category_files:
                continue
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†…å®¹ã‚’ç”Ÿæˆ
            content = f"# {category}\n\n"
            content += f"ã“ã®ã‚«ãƒ†ã‚´ãƒªã«ã¯ {len(category_files)} ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã™ã€‚\n\n"
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            if info['keywords']:
                content += f"**é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: {', '.join(info['keywords'])}\n\n"
            
            content += "## ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸€è¦§\n\n"
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆï¼ˆã‚¹ã‚³ã‚¢é †ï¼‰
            category_files.sort(key=lambda x: x[1].get('score', 0), reverse=True)
            
            for filepath, file_info in category_files:
                title = file_info['metadata'].get('title', os.path.basename(filepath))
                filename = self.clean_filename(os.path.basename(filepath))
                tags = ' '.join([f'#{tag}' for tag in file_info.get('tags', [])[:3]])
                
                content += f"- [[{filename}|{title}]] {tags}\n"
            
            with open(index_path, 'w', encoding='utf-8') as f:
                f.write(content)
    
    def create_yoshinani_index(self):
        """ã‚ˆã—ãªã«å¯¾å¿œã®ç‰¹åˆ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ"""
        yoshinani_path = os.path.join(KNOWLEDGE_BASE_DIR, "01_ã‚ˆã—ãªã«å¯¾å¿œ", "ã‚ˆã—ãªã«å¯¾å¿œ_å®Œå…¨ã‚¬ã‚¤ãƒ‰.md")
        
        content = """# ã‚ˆã—ãªã«å¯¾å¿œ å®Œå…¨ã‚¬ã‚¤ãƒ‰

## ğŸ“– ã‚ˆã—ãªã«å¯¾å¿œã¨ã¯

**ã‚ˆã—ãªã«å¯¾å¿œ = ç›¸æ‰‹ç›®ç·šã§"æ‰‹é–“"ã‚„"é¢å€’"ã‚’å·»ãå–ã‚‹åŠ›**

### 3ã¤ã®æŸ±

1. **ä¿¡é ¼æ§‹ç¯‰åŠ›** - ã€Œè¿…é€Ÿãƒ»å³å®ˆãƒ»èª å®Ÿãƒ»å®‰å¿ƒãƒ»æ¸©åº¦ã€
2. **æƒ…å ±ç·¨é›†åŠ›** - ã€Œç›¸æ‰‹ã®æ„å›³ã‚’èª­ã¿è§£ãã€2.3æ‰‹å…ˆå›ã‚Šã—ãŸå¯¾å¿œã€
3. **å½“äº‹è€…æ„è­˜** - ã€Œè‡ªåˆ†ã”ã¨ã¨ã—ã¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’æ‰ãˆã‚‹ã€

## ğŸ“š å­¦ç¿’ãƒ‘ã‚¹

### åŸºç¤ç·¨
1. [[ç¬¬1å› ã‚ˆã—ãªã«åŸºç¤ç†è§£è¬›åº§ ã€œã‚ˆã—ãªã«ã£ã¦ä½•ï¼Ÿã€œ.md|ç¬¬1å› ã‚ˆã—ãªã«åŸºç¤ç†è§£è¬›åº§]]
2. [[ã‚ˆã—ãªã«åŠ›ã®æœ¬è³ª.md|ã‚ˆã—ãªã«åŠ›ã®æœ¬è³ª]]
3. [[ã‚ˆã—ãªã«å¯¾å¿œã‚’æ§‹é€ åŒ–.md|ã‚ˆã—ãªã«å¯¾å¿œã‚’æ§‹é€ åŒ–]]

### å®Ÿè·µç·¨
1. [[ã‚ˆã—ãªã«å¯¾å¿œé‰„æ¿ãƒã‚¤ãƒ³ãƒˆï¼ˆã‚„ã‚Šå–ã‚Šç·¨ï¼‰.md|ã‚ˆã—ãªã«å¯¾å¿œé‰„æ¿ãƒã‚¤ãƒ³ãƒˆï¼ˆã‚„ã‚Šå–ã‚Šç·¨ï¼‰]]
2. [[ã‚ˆã—ãªã«å¯¾å¿œé‰„æ¿ãƒã‚¤ãƒ³ãƒˆï¼ˆå®Ÿè£…ç·¨ï¼‰.md|ã‚ˆã—ãªã«å¯¾å¿œé‰„æ¿ãƒã‚¤ãƒ³ãƒˆï¼ˆå®Ÿè£…ç·¨ï¼‰]]
3. [[å®Ÿæ¡ˆä»¶ã®ã‚ˆã—ãªã«å¯¾å¿œä¾‹ï¼ˆææ¡ˆã¨å ±å‘Šæ›¸ï¼‰.md|å®Ÿæ¡ˆä»¶ã®ã‚ˆã—ãªã«å¯¾å¿œä¾‹]]

### å¿œç”¨ç·¨
1. [[ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‚ˆã—ãªã«å¯¾å¿œï¼ˆãã®ï¼‘ï¼‰.md|ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‚ˆã—ãªã«å¯¾å¿œ]]
2. [[è«‹æ±‚æ›¸ã‚’å‡ºã™æ™‚ã®ã‚ˆã—ãªã«ãƒã‚¤ãƒ³ãƒˆ.md|è«‹æ±‚æ›¸ã‚’å‡ºã™æ™‚ã®ã‚ˆã—ãªã«ãƒã‚¤ãƒ³ãƒˆ]]
3. [[é€²æ—ç®¡ç†ã‚’ã¾ã‚‹ã£ã¨å·»ãå–ã‚‹ã€ã‚ˆã—ãªã«ç®¡ç†ã‚·ãƒ¼ãƒˆã€Ÿ.md|ã‚ˆã—ãªã«ç®¡ç†ã‚·ãƒ¼ãƒˆ]]

## ğŸ› ï¸ ãƒ„ãƒ¼ãƒ«ãƒ»ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

- [[ã‚ˆã—ãªã«ãƒã‚§ãƒƒã‚¯ã‚·ãƒ¼ãƒˆ.md|ã‚ˆã—ãªã«ãƒã‚§ãƒƒã‚¯ã‚·ãƒ¼ãƒˆ]]
- [[ã‚ˆã—ãªã«ãƒã‚§ãƒƒã‚¯ã‚·ãƒ¼ãƒˆï¼ˆæ¯é€±ï¼‰.md|ã‚ˆã—ãªã«ãƒã‚§ãƒƒã‚¯ã‚·ãƒ¼ãƒˆï¼ˆæ¯é€±ï¼‰]]
- [[ã‚ˆã—ãªã«ãƒã‚§ãƒƒã‚¯ã‚·ãƒ¼ãƒˆï¼ˆæ¯æœˆï¼‰.md|ã‚ˆã—ãªã«ãƒã‚§ãƒƒã‚¯ã‚·ãƒ¼ãƒˆï¼ˆæ¯æœˆï¼‰]]
- [[ã‚ˆã—ãªã«ãƒã‚§ãƒƒã‚¯ã‚·ãƒ¼ãƒˆï¼ˆæ¡ˆä»¶å¯¾å¿œï¼‰.md|ã‚ˆã—ãªã«ãƒã‚§ãƒƒã‚¯ã‚·ãƒ¼ãƒˆï¼ˆæ¡ˆä»¶å¯¾å¿œï¼‰]]

## ğŸ’¡ é‡è¦ãªè€ƒãˆæ–¹

### æ„Ÿè¬Ã—ç›¸æ‰‹ç›®ç·šã®PDCA
- ç›¸æ‰‹ã®ç«‹å ´ãƒ»ç›®çš„ãƒ»æŠ±ãˆã¦ã„ã‚‹è² æ‹…ã‚’èª­ã¿å–ã‚Šã€**ã“ã¡ã‚‰ã‹ã‚‰å‹•ãå§¿å‹¢**
- ã€Œä½•ãŒæ‰‹é–“ã‹ï¼Ÿã€ã‚’æƒ³åƒã—ã€å·»ãå–ã‚‹æ„è­˜ã‚’å¸¸ã«æŒã¤
- ãã®ç¹°ã‚Šè¿”ã—ãŒã€ä»–è€…ã«ã¯ãªã„"ä½™è£•"ã¨"è‡ªä¿¡"ã«ã¤ãªãŒã‚‹

### ç–²å¼Šã—ãªã„ãŸã‚ã®"ç·šå¼•ãåŠ›"
- å¿…è¦æœ€ä½é™ã®ã‚„ã‚Šã¨ã‚Šï¼ˆçœã‚¨ãƒå¯¾å¿œï¼‰
- ã€Œã“ã®ä¼šç¤¾ã¨1å¹´å¾Œã‚‚ä»˜ãåˆã„ãŸã„ã‹ï¼Ÿã€ã§åˆ¤æ–­
- ãƒ•ãƒ«ã‚³ãƒŸãƒƒãƒˆã™ã‚‹ç›¸æ‰‹ã¯è‡ªåˆ†ã§é¸ã¶

## ğŸ“ˆ æˆé•·ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **æ„Ÿè¬ã®æ°—æŒã¡ã‚’æŒã¤** - ã™ã¹ã¦ã®èµ·ç‚¹
2. **ç›¸æ‰‹ç›®ç·šã§è€ƒãˆã‚‹** - ä½•ãŒæ‰‹é–“ã‹ï¼Ÿã‚’æƒ³åƒ
3. **å…ˆå›ã‚Šã—ã¦å‹•ã** - 2,3æ‰‹å…ˆã‚’èª­ã‚€
4. **PDCAã‚’å›ã™** - å¸¸ã«æ”¹å–„ã—ç¶šã‘ã‚‹
5. **è‡ªä¿¡ã‚’æŒã¤** - ç©ã¿é‡ã­ãŒä½™è£•ã‚’ç”Ÿã‚€

---

ã“ã®ã‚¬ã‚¤ãƒ‰ã¯éšæ™‚æ›´æ–°ã•ã‚Œã¾ã™ã€‚
æœ€çµ‚æ›´æ–°: """ + datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        with open(yoshinani_path, 'w', encoding='utf-8') as f:
            f.write(content)
    
    def generate_report(self):
        """å‡¦ç†ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        report_path = os.path.join(KNOWLEDGE_BASE_DIR, "00_Index", "Integration_Report.md")
        
        content = f"""# Knowledge Base Integration Report

Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Summary

- Total files processed: {len(self.file_index)}
- Files reorganized: {len([f for f in self.file_index.values() if 'duplicate_of' not in f])}
- Duplicates found: {len([f for f in self.file_index.values() if 'duplicate_of' in f])}

## Category Distribution

"""
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ
        category_dist = {}
        for file_info in self.file_index.values():
            if 'duplicate_of' not in file_info:
                category = file_info['category']
                category_dist[category] = category_dist.get(category, 0) + 1
        
        for category, count in sorted(category_dist.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / sum(category_dist.values())) * 100
            content += f"- {category}: {count} files ({percentage:.1f}%)\n"
        
        content += "\n## Processing Details\n\n"
        content += f"- Backup location: {BACKUP_DIR}\n"
        content += f"- Knowledge base location: {KNOWLEDGE_BASE_DIR}\n"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"\nReport saved to: {report_path}")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("Comprehensive Knowledge Integration")
    print("=" * 60)
    
    integrator = KnowledgeIntegrator()
    
    # 1. ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä½œæˆ
    integrator.create_directories()
    
    # 2. æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
    integrator.backup_existing_files()
    
    # 3. ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³
    file_count = integrator.scan_all_files()
    
    if file_count == 0:
        print("No files found to process.")
        return
    
    # 4. ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡
    integrator.categorize_files()
    
    # 5. ã‚¿ã‚°ã¨é–¢é€£æ€§ã‚’æŠ½å‡º
    integrator.extract_tags_and_relations()
    
    # 6. é‡è¤‡ã‚’å‡¦ç†
    integrator.remove_duplicates()
    
    # 7. ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†ç·¨æˆ
    integrator.reorganize_files()
    
    # 8. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    integrator.create_index_files()
    
    # 9. ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    integrator.generate_report()
    
    print("\n" + "=" * 60)
    print("Integration completed successfully!")
    print("=" * 60)

if __name__ == "__main__":
    main()